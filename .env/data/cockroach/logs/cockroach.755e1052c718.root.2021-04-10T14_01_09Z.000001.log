I210410 14:01:09.675876 1 util/log/sync_buffer.go:195 ⋮ [config] file created at: 2021/04/10 14:01:09
I210410 14:01:09.675892 1 util/log/sync_buffer.go:195 ⋮ [config] running on machine: ‹755e1052c718›
I210410 14:01:09.675905 1 util/log/sync_buffer.go:195 ⋮ [config] binary: CockroachDB CCL v20.2.7 (x86_64-unknown-linux-gnu, built 2021/03/29 17:52:00, go1.13.14)
I210410 14:01:09.675914 1 util/log/sync_buffer.go:195 ⋮ [config] arguments: ‹[/cockroach/cockroach start-single-node --http-port 26256 --insecure]›
I210410 14:01:09.675932 1 util/log/sync_buffer.go:195 ⋮ [config] line format: [IWEF]yymmdd hh:mm:ss.uuuuuu goid file:line msg utf8=✓
W210410 14:01:09.675771 1 cli/start.go:1143 ⋮ ALL SECURITY CONTROLS HAVE BEEN DISABLED!

This mode is intended for non-production testing only.

In this mode:
- Your cluster is open to any client that can access ‹any of your IP addresses›.
- Intruders with access to your machine or network can observe client-server traffic.
- Intruders can log in without password and read or write any data in the cluster.
- Intruders can consume all your server's resources and cause unavailability.
I210410 14:01:09.676067 1 cli/start.go:1153 ⋮ To start a secure server without mandating TLS for clients,
consider --accept-sql-without-tls instead. For other options, see:

- ‹https://go.crdb.dev/issue-v/53404/v20.2›
- https://www.cockroachlabs.com/docs/v20.2/secure-a-cluster.html
I210410 14:01:09.676650 1 server/status/recorder.go:605 ⋮ ‹available memory from cgroups (8.0 EiB) is unsupported, using system memory 7.6 GiB instead:›
W210410 14:01:09.676727 1 cli/start.go:987 ⋮ ‹Using the default setting for --cache (128 MiB).›
‹  A significantly larger value is usually needed for good performance.›
‹  If you have a dedicated server a reasonable setting is --cache=.25 (1.9 GiB).›
I210410 14:01:09.677199 1 server/status/recorder.go:605 ⋮ ‹available memory from cgroups (8.0 EiB) is unsupported, using system memory 7.6 GiB instead:›
I210410 14:01:09.677245 1 cli/start.go:1168 ⋮ ‹CockroachDB CCL v20.2.7 (x86_64-unknown-linux-gnu, built 2021/03/29 17:52:00, go1.13.14)›
I210410 14:01:09.679337 1 server/status/recorder.go:605 ⋮ ‹available memory from cgroups (8.0 EiB) is unsupported, using system memory 7.6 GiB instead:›
I210410 14:01:09.679421 1 server/config.go:428 ⋮ system total memory: ‹7.6 GiB›
I210410 14:01:09.679475 1 server/config.go:430 ⋮ server configuration:
‹max offset             500000000›
‹cache size             128 MiB›
‹SQL memory pool size   1.9 GiB›
‹scan interval          10m0s›
‹scan min idle time     10ms›
‹scan max idle time     1s›
‹event log enabled      true›
I210410 14:01:09.679594 1 cli/start.go:965 ⋮ using local environment variables: ‹COCKROACH_CHANNEL=official-docker›
I210410 14:01:09.679640 1 cli/start.go:972 ⋮ process identity: ‹uid 0 euid 0 gid 0 egid 0›
I210410 14:01:09.683746 1 cli/start.go:511 ⋮ GEOS loaded from directory ‹/usr/local/lib/cockroach›
I210410 14:01:09.683794 1 cli/start.go:516 ⋮ starting cockroach node
I210410 14:01:10.302409 29 server/server.go:790 ⋮ [n?] monitoring forward clock jumps based on server.clock.forward_jump_check_enabled
I210410 14:01:10.805432 29 server/config.go:619 ⋮ [n?] 1 storage engine‹› initialized
I210410 14:01:10.805536 29 server/config.go:622 ⋮ [n?] ‹Pebble cache size: 128 MiB›
I210410 14:01:10.805589 29 server/config.go:622 ⋮ [n?] ‹store 0: RocksDB, max size 0 B, max open file limit 1043576›
I210410 14:01:10.907966 45 server/server.go:1424 ⋮ [n?] connecting to gossip network to verify cluster ID ‹"dd9dc586-c756-4f4b-a9e5-9e23cf559418"›
W210410 14:01:10.907977 29 cli/start.go:911 ⋮ neither --listen-addr nor --advertise-addr was specified.
The server will advertise ‹"755e1052c718"› to other nodes, is this routable?

Consider using:
- for local-only servers:  --listen-addr=localhost
- for multi-node clusters: --advertise-addr=<host/IP addr>
I210410 14:01:10.908159 29 gossip/gossip.go:403 ⋮ [n1] NodeDescriptor set to ‹node_id:1 address:<network_field:"tcp" address_field:"755e1052c718:26257" > attrs:<> locality:<> ServerVersion:<major_val:20 minor_val:2 patch:0 unstable:0 > build_tag:"v20.2.7" started_at:1618063270908152686 cluster_name:"" sql_address:<network_field:"tcp" address_field:"755e1052c718:26257" >›
W210410 14:01:10.917870 235 kv/kvserver/replica_range_lease.go:556 ⋮ [n1,s1,r6/1:‹/Table/{SystemCon…-11}›] can't determine lease status of (n1,s1):1 due to node liveness error: node not in the liveness table
(1) attached stack trace
  -- stack trace:
  | github.com/cockroachdb/cockroach/pkg/kv/kvserver.init
  | 	/go/src/github.com/cockroachdb/cockroach/pkg/kv/kvserver/node_liveness.go:45
  | runtime.doInit
  | 	/usr/local/go/src/runtime/proc.go:5228
  | runtime.doInit
  | 	/usr/local/go/src/runtime/proc.go:5223
  | runtime.doInit
  | 	/usr/local/go/src/runtime/proc.go:5223
  | runtime.doInit
  | 	/usr/local/go/src/runtime/proc.go:5223
  | runtime.doInit
  | 	/usr/local/go/src/runtime/proc.go:5223
  | runtime.main
  | 	/usr/local/go/src/runtime/proc.go:190
  | runtime.goexit
  | 	/usr/local/go/src/runtime/asm_amd64.s:1357
Wraps: (2) node not in the liveness table
Error types: (1) *withstack.withStack (2) *errutil.leafError
I210410 14:01:10.918071 45 server/server.go:1427 ⋮ [n1] node connected via gossip
W210410 14:01:10.918092 235 kv/kvserver/store.go:1691 ⋮ [n1,s1,r6/1:‹/Table/{SystemCon…-11}›] could not gossip system config: ‹[NotLeaseHolderError] r6: replica (n1,s1):1 not lease holder; lease holder unknown›
(1) ‹[NotLeaseHolderError] r6: replica (n1,s1):1 not lease holder; lease holder unknown›
Error types: (1) *roachpb.NotLeaseHolderError
I210410 14:01:10.918652 29 server/node.go:430 ⋮ [n1] initialized store [n1,s1]: disk (capacity=196 GiB, available=4.3 GiB, used=5.4 MiB, logicalBytes=25 MiB), ranges=57, leases=0, queries=0.00, writes=0.00, bytesPerReplica={p10=0.00 p25=0.00 p50=0.00 p75=333.00 p90=42964.00 pMax=25878844.00}, writesPerReplica={p10=0.00 p25=0.00 p50=0.00 p75=0.00 p90=0.00 pMax=0.00}
I210410 14:01:10.918794 29 kv/kvserver/stores.go:236 ⋮ [n1] read 0 node addresses from persistent storage
W210410 14:01:10.970786 235 kv/kvserver/store.go:1691 ⋮ [n1,s1,r6/1:‹/Table/{SystemCon…-11}›] could not gossip system config: ‹[NotLeaseHolderError] r6: replica (n1,s1):1 not lease holder; lease holder unknown›
(1) ‹[NotLeaseHolderError] r6: replica (n1,s1):1 not lease holder; lease holder unknown›
Error types: (1) *roachpb.NotLeaseHolderError
W210410 14:01:11.080896 235 kv/kvserver/store.go:1691 ⋮ [n1,s1,r6/1:‹/Table/{SystemCon…-11}›] could not gossip system config: ‹[NotLeaseHolderError] r6: replica (n1,s1):1 not lease holder; lease holder unknown›
(1) ‹[NotLeaseHolderError] r6: replica (n1,s1):1 not lease holder; lease holder unknown›
Error types: (1) *roachpb.NotLeaseHolderError
I210410 14:01:11.082169 29 server/node.go:489 ⋮ [n1] started with engine type ‹2›
I210410 14:01:11.082247 29 server/node.go:491 ⋮ [n1] started with attributes ‹[]›
I210410 14:01:11.082372 29 server/goroutinedumper/goroutinedumper.go:120 ⋮ [n1] writing goroutine dumps to ‹/cockroach/cockroach-data/logs/goroutine_dump›
I210410 14:01:11.082407 29 server/heapprofiler/heapprofiler.go:49 ⋮ [n1] writing go heap profiles to ‹/cockroach/cockroach-data/logs/heap_profiler› at least every 1h0m0s
I210410 14:01:11.082438 29 server/heapprofiler/cgoprofiler.go:53 ⋮ [n1] to enable jmalloc profiling: "export MALLOC_CONF=prof:true" or "ln -s prof:true /etc/malloc.conf"
I210410 14:01:11.082455 29 server/heapprofiler/statsprofiler.go:54 ⋮ [n1] writing memory stats to ‹/cockroach/cockroach-data/logs/heap_profiler› at last every 1h0m0s
I210410 14:01:11.082538 29 server/server.go:1544 ⋮ [n1] starting http server at ‹[::]:26256› (use: ‹755e1052c718:26256›)
I210410 14:01:11.082577 29 server/server.go:1551 ⋮ [n1] starting grpc/postgres server at ‹[::]:26257›
I210410 14:01:11.082613 29 server/server.go:1552 ⋮ [n1] advertising CockroachDB node at ‹755e1052c718:26257›
I210410 14:01:11.907567 29 sql/sqlliveness/slinstance/slinstance.go:252 ⋮ [n1] starting SQL liveness instance
I210410 14:01:11.908826 29 server/server_sql.go:800 ⋮ [n1] done ensuring all necessary migrations have run
I210410 14:01:11.908917 29 server/server.go:1887 ⋮ [n1] serving sql connections
I210410 14:01:11.909419 29 cli/start.go:677 ⋮ [config] clusterID: ‹dd9dc586-c756-4f4b-a9e5-9e23cf559418›
I210410 14:01:11.909562 29 cli/start.go:687 ⋮ node startup completed:
CockroachDB node starting at 2021-04-10 14:01:11.909163206 +0000 UTC (took 2.2s)
build:               CCL v20.2.7 @ 2021/03/29 17:52:00 (go1.13.14)
webui:               ‹http://755e1052c718:26256›
sql:                 ‹postgresql://root@755e1052c718:26257?sslmode=disable›
RPC client flags:    ‹/cockroach/cockroach <client cmd> --host=755e1052c718:26257 --insecure›
logs:                ‹/cockroach/cockroach-data/logs›
temp dir:            ‹/cockroach/cockroach-data/cockroach-temp005515669›
external I/O path:   ‹/cockroach/cockroach-data/extern›
store[0]:            ‹path=/cockroach/cockroach-data›
storage engine:      pebble
status:              restarted pre-existing node
clusterID:           ‹dd9dc586-c756-4f4b-a9e5-9e23cf559418›
nodeID:              1
I210410 14:01:11.909699 457 jobs/job_scheduler.go:349 ⋮ [n1] waiting 2m0s before scheduled jobs daemon start
I210410 14:01:11.911942 331 sql/temporary_schema.go:510 ⋮ [n1] running temporary object cleanup background job
I210410 14:01:11.916360 458 server/server_update.go:55 ⋮ [n1] no need to upgrade, cluster already at the newest version
I210410 14:01:12.332533 331 sql/temporary_schema.go:545 ⋮ [n1] found 0 temporary schemas
I210410 14:01:12.332584 331 sql/temporary_schema.go:548 ⋮ [n1] early exiting temporary schema cleaner as no temporary schemas were found
I210410 14:01:12.332606 331 sql/temporary_schema.go:549 ⋮ [n1] completed temporary object cleanup job
I210410 14:01:12.332626 331 sql/temporary_schema.go:627 ⋮ [n1] temporary object cleaner next scheduled to run at 2021-04-10 14:31:11.907971781 +0000 UTC
I210410 14:01:12.745235 327 sql/event_log.go:162 ⋮ [n1] Event: ‹"node_restart"›, target: 1, info: ‹{Descriptor:{NodeID:1 Address:755e1052c718:26257 Attrs: Locality: ServerVersion:20.2 BuildTag:v20.2.7 StartedAt:1618063270908152686 LocalityAddress:[] ClusterName: SQLAddress:755e1052c718:26257} ClusterID:dd9dc586-c756-4f4b-a9e5-9e23cf559418 StartedAt:1618063270908152686 LastUp:1618062757436374825}›
I210410 14:01:12.834703 329 sql/sqlliveness/slstorage/slstorage.go:348 ⋮ [n1] inserted sqlliveness session ‹7174fd32ce684848b1de8121f41d3e4b›
I210410 14:01:12.834843 329 sql/sqlliveness/slinstance/slinstance.go:143 ⋮ [n1] created new SQL liveness session ‹7174fd32ce684848b1de8121f41d3e4b›
I210410 14:01:12.841787 141 gossip/gossip.go:1508 ⋮ [n1] node has connected to cluster via gossip
I210410 14:01:12.913055 141 kv/kvserver/stores.go:255 ⋮ [n1] wrote 0 node addresses to persistent storage
W210410 14:01:18.198539 215 kv/kvserver/store_raft.go:493 ⋮ [n1,s1,r2/1:‹/System/NodeLiveness{-Max}›] handle raft ready: 1.3s [applied=1, batches=1, state_assertions=0]
W210410 14:01:19.186617 202 kv/kvserver/store_raft.go:493 ⋮ [n1,s1,r35/1:‹/Table/{39-53}›] handle raft ready: 0.6s [applied=1, batches=1, state_assertions=0]
W210410 14:01:20.219972 212 kv/kvserver/store_raft.go:493 ⋮ [n1,s1,r56/1:‹/Table/6{0-1}›] handle raft ready: 1.1s [applied=2, batches=1, state_assertions=0]
I210410 14:01:21.084017 295 server/status/runtime.go:525 ⋮ [n1] runtime stats: 132 MiB RSS, 207 goroutines, 24 MiB/33 MiB/41 MiB GO alloc/idle/total, 10 MiB/21 MiB CGO alloc/total, 0.0 CGO/sec, 0.0/0.0 %(u/s)time, 0.0 %gc (13x), 8.3 KiB/14 KiB (r/w)net
W210410 14:01:28.046504 192 kv/kvserver/store_raft.go:493 ⋮ [n1,s1,r21/1:‹/Table/2{5-6}›] handle raft ready: 0.9s [applied=2, batches=1, state_assertions=0]
I210410 14:01:31.087718 295 server/status/runtime.go:525 ⋮ [n1] runtime stats: 135 MiB RSS, 206 goroutines, 22 MiB/34 MiB/44 MiB GO alloc/idle/total, 11 MiB/22 MiB CGO alloc/total, 0.9 CGO/sec, 2.6/0.9 %(u/s)time, 0.0 %gc (1x), 2.5 KiB/11 KiB (r/w)net
I210410 14:01:31.088171 290 kv/kvserver/store.go:2625 ⋮ [n1,s1] sstables (read amplification = 1):
‹6 [ 1M 1 ]: 1M›
I210410 14:01:31.088764 290 kv/kvserver/store.go:2626 ⋮ [n1,s1] ‹›
‹__level_____count____size___score______in__ingest(sz_cnt)____move(sz_cnt)___write(sz_cnt)____read___r-amp___w-amp›
‹    WAL         2   550 K       -   547 K       -       -       -       -   550 K       -       -       -     1.0›
‹      0         0     0 B    0.00     0 B     0 B       0     0 B       0     0 B       0     0 B       0     0.0›
‹      1         0     0 B    0.00     0 B     0 B       0     0 B       0     0 B       0     0 B       0     0.0›
‹      2         0     0 B    0.00     0 B     0 B       0     0 B       0     0 B       0     0 B       0     0.0›
‹      3         0     0 B    0.00     0 B     0 B       0     0 B       0     0 B       0     0 B       0     0.0›
‹      4         0     0 B    0.00     0 B     0 B       0     0 B       0     0 B       0     0 B       0     0.0›
‹      5         0     0 B    0.00     0 B     0 B       0     0 B       0     0 B       0     0 B       0     0.0›
‹      6         1   1.1 M       -    74 K     0 B       0     0 B       0   1.1 M       1   1.2 M       1    15.1›
‹  total         1   1.1 M       -   550 K     0 B       0     0 B       0   1.6 M       1   1.2 M       1     3.0›
‹  flush         0›
‹compact         1     0 B          (size == estimated-debt)›
‹ memtbl         2   2.3 M›
‹zmemtbl         0     0 B›
‹   ztbl         0     0 B›
‹ bcache        30   972 K   93.9%  (score == hit-rate)›
‹ tcache         1   616 B   99.8%  (score == hit-rate)›
‹ titers         0›
‹ filter         -       -   87.7%  (score == utility)›
I210410 14:01:31.910779 328 sql/sqlliveness/slstorage/slstorage.go:326 ⋮ [n1] deleted 2 expired SQL liveness sessions
W210410 14:01:32.692889 202 kv/kvserver/store_raft.go:493 ⋮ [n1,s1,r35/1:‹/Table/{39-53}›] handle raft ready: 0.8s [applied=1, batches=1, state_assertions=0]
W210410 14:01:32.940024 226 kv/kvserver/store_raft.go:493 ⋮ [n1,s1,r3/1:‹/System/{NodeLive…-tsd}›] handle raft ready: 1.0s [applied=1, batches=1, state_assertions=0]
W210410 14:01:32.940070 224 kv/kvserver/store_raft.go:493 ⋮ [n1,s1,r14/1:‹/Table/1{8-9}›] handle raft ready: 0.8s [applied=2, batches=1, state_assertions=0]
W210410 14:01:32.944709 220 kv/kvserver/store_raft.go:493 ⋮ [n1,s1,r4/1:‹/System{/tsd-tse}›] handle raft ready: 1.0s [applied=1, batches=1, state_assertions=0]
I210410 14:01:33.306652 1 cli/start.go:736 ⋮ received signal 'terminated'
I210410 14:01:33.306761 1 cli/start.go:821 ⋮ initiating graceful shutdown of server
I210410 14:01:33.663173 838 server/drain.go:174 ⋮ [server drain process] drain remaining: 2
I210410 14:01:33.663330 838 server/drain.go:176 ⋮ [server drain process] drain details: descriptor leases: 1, liveness record: 1
W210410 14:01:35.220507 210 kv/kvserver/store_raft.go:493 ⋮ [n1,s1,r28/1:‹/Table/3{2-3}›] handle raft ready: 0.7s [applied=1, batches=1, state_assertions=0]
I210410 14:01:35.220752 838 server/drain.go:174 ⋮ [server drain process] drain remaining: 0
I210410 14:01:35.220888 838 util/stop/stopper.go:563 ⋮ [server drain process] quiescing
W210410 14:01:35.221046 329 sql/sqlliveness/slinstance/slinstance.go:182 ⋮ [n1] exiting heartbeat loop
W210410 14:01:35.221112 452 jobs/registry.go:675 ⋮ canceling all adopted jobs due to stopper quiescing
I210410 14:01:35.221222 719 kv/kvserver/queue.go:1187 ⋮ [n1,replicate] purgatory is now empty
I210410 14:01:35.372850 193 kv/kvserver/queue.go:582 ⋮ [n1,s1,r102/1:‹/Table/7{3-4}›] rate limited in MaybeAdd (replicate): ‹node unavailable; try another peer›
I210410 14:01:35.373365 193 kv/kvserver/queue.go:582 ⋮ [n1,s1,r102/1:‹/Table/7{3-4}›] rate limited in MaybeAdd (merge): ‹node unavailable; try another peer›
I210410 14:01:35.511939 1 cli/start.go:873 ⋮ server drained and shutdown completed
